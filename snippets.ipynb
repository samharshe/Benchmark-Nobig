{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the important snippets I used to build my benchmark models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVGNN(nn.Module):\n",
    "    # define the model\n",
    "    def __init__(self) -> None:\n",
    "        super(EVGNN1, self).__init__()\n",
    "        # size of the embedding space\n",
    "        self.embed_dim = 32\n",
    "        \n",
    "        # map the atomic number into the embedding space\n",
    "        self.embedding = nn.Embedding(118, self.embed_dim)\n",
    "        # message passing\n",
    "        self.mp1 = EVMPLayer(self.embed_dim) # or SquareEVMPLayer or RestrictedEVMPLayer\n",
    "        self.mp2 = EVMPLayer(self.embed_dim) # or SquareEVMPLayer or RestrictedEVMPLayer\n",
    "        self.prediction1 = RowWiseFCL(self.embed_dim, 8) # 32d -> 8d\n",
    "        self.prediction2 = RowWiseFCL(8, 1) # 8d -> 1d\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, data = Data) -> float:\n",
    "        # data.x was floats: had to convert to long\n",
    "        x = self.embedding(data.x.long())\n",
    "        # message pass on the embedding vector and on the edges x2\n",
    "        # normalized after message passgin but nowhere else\n",
    "        x = self.mp1(x, data.e)\n",
    "        x = F.normalize(x, p=1, dim=0)\n",
    "        x = self.mp2(x, data.e)\n",
    "        x = F.normalize(x, p=1, dim=0)\n",
    "        x = self.prediction1(x)\n",
    "        x = self.prediction2(x)\n",
    "        # use scatter\n",
    "        U_hat = torch.sum(x)\n",
    "        return U_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVMPLayer(nn.Module):\n",
    "    def __init__(self, embed_dim: int) -> None:\n",
    "        super(EVMPLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        # maybe made it too slow\n",
    "        # GeLU\n",
    "        self.act = nn.Tanh()\n",
    "        # message: source node, destination node, edge\n",
    "        message_input_size = 2 * embed_dim + 1\n",
    "        \n",
    "        # take in a message tensor of size 2 * embed_dim + 1 and get out a new h_i of size embed_dim\n",
    "        self.message_mlp = nn.Sequential(nn.Linear(message_input_size, embed_dim), self.act)\n",
    "        \n",
    "        # take in a message tensor of size embed_dim and an original h_i of size embed_dim and get out a new h_i of size embed_dim\n",
    "        self.update_node_mlp = nn.Sequential(nn.Linear(2 * embed_dim, embed_dim), self.act)\n",
    "                \n",
    "    # helper function to combine all the relevant tensors into one \n",
    "    def make_message(self, source_tensor: int, target_tensor: int, distance: float) -> Tensor:\n",
    "        combined_tensor = torch.cat((source_tensor.view(-1), target_tensor.view(-1), torch.Tensor([distance])))\n",
    "        return self.message_mlp(combined_tensor)\n",
    "    \n",
    "    # combine the input tensor with the message tensor and pass through the mlp\n",
    "    def update_node(self, node_tensor: Tensor, message_tensor: Tensor) -> Tensor:\n",
    "        combined_tensor = torch.cat((node_tensor, message_tensor)).view(1,-1)\n",
    "        return self.update_node_mlp(combined_tensor)\n",
    "    \n",
    "    def forward(self, embed_tensor: Tensor, edge_distances: Tensor) -> Tensor:\n",
    "        new_embed_tensor = torch.zeros_like(embed_tensor)\n",
    "        # for each molecule in the dataset\n",
    "        for ix, source in enumerate(embed_tensor):\n",
    "            # create a tensor that tracks the sum of the messages\n",
    "            message_sum = torch.zeros_like(source)\n",
    "            # for each other molecule in the dataset\n",
    "            for jx, target in enumerate(embed_tensor):\n",
    "                if ix != jx:\n",
    "                    # add the message tensor between them to the sum tensor\n",
    "                    # this next line depends on the kind of message-passing we’re doing\n",
    "                    # radial basis function\n",
    "                    # radius graph in torch_geometric\n",
    "                    message_sum += self.make_message(source, target, edge_distances[ix,jx].item())\n",
    "            # update the tensor that keeps track of all molecule embeddings by making its row ix the new embedding of the molecule\n",
    "            new_embed_tensor[ix] = self.update_node(source, message_sum)\n",
    "        \n",
    "        return new_embed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty stupid\n",
    "# was meant to be a convenient wrapper for all the Molecule objects, but I didn’t end up using the DataLoader, so this was pointless\n",
    "class MoleculesDataset(Dataset):\n",
    "    def __init__(self, data: List[Data]) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Data:\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all the edge distances once and save them with the object so that we don’t have to recalculate them each pass through\n",
    "# saved all these to machinee instead of loading in PyTorch_Geometric’s built-in QM9 every time because I only wanted position and U_0\n",
    "class FullyConnectedData():\n",
    "    def __init__(self, x: Tensor, pos: Tensor, y: Tensor) -> None:\n",
    "        self.x = x\n",
    "        self.pos = pos\n",
    "        self.y = y\n",
    "        self.e = make_edge_distances(pos)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return x.size(0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"x: {self.x.size()} | pos: {self.pos.size()} | e: {self.e.size()} | y = {self.y}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really sloppy with hyperparameters\n",
    "# advice more than welcome\n",
    "lr = 0.001 # I think way too high by the end\n",
    "# lr decay scheduler\n",
    "# cosine annealing scheduler\n",
    "# step function scheduler\n",
    "batch_size = 32\n",
    "fcle = EVGNN1() # or whichever model\n",
    "optim = torch.optim.Adam(fcle.parameters(), lr=lr) # popular, obviously, but didn’t have very good reason to choose it\n",
    "loss_fn = nn.MSELoss() # probably wrong\n",
    "fcle_losses = []\n",
    "\n",
    "fcle.train()\n",
    "# dataset broken into 24 chunks in storage so that it will load in\n",
    "for i in range(24):\n",
    "    train_dataset = torch.load(f'Zebra/T{i}.pt')\n",
    "\n",
    "    # comput the loss over the entire batch\n",
    "    loss = torch.Tensor([0])\n",
    "    loss.requires_grad = True\n",
    "\n",
    "    # used to batch training\n",
    "    j = 0\n",
    "    for data in train_dataset:\n",
    "        # calculate prediction\n",
    "        U_hat = fcle(data)\n",
    "        # target\n",
    "        U = data.y\n",
    "        # add the loss of this single example to the loss tensor for the whole batch\n",
    "        loss = torch.add(loss,loss_fn(U_hat, U))\n",
    "        \n",
    "        j += 1\n",
    "        # backprop if we’ve completed a batch or a tranche\n",
    "        if j % batch_size == 0 or j == len(train_dataset)-1:\n",
    "            # make sure we’re not accumulating gradients\n",
    "            optim.zero_grad()\n",
    "            # chain rule\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optim.step()\n",
    "            # add the losses of the model to the training loss log\n",
    "            fcle_losses.append(loss.item())\n",
    "            # reset the loss tensor\n",
    "            loss = torch.Tensor([0])\n",
    "            loss.requires_grad = True\n",
    "\n",
    "            print(f'DATA TRANCHE {i} | {j} EXAMPLES COMPLETE | PRIOR BATCH LOSS {fcle_losses[-1]}')\n",
    "            \n",
    "# save the weights of our model\n",
    "torch.save(fcle,'Zebra/fcle.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "fcle_losses = []\n",
    "fcse_losses = []\n",
    "pcle_losses = []\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    # broken into two chunks\n",
    "    for i in range(2):\n",
    "        test_dataset = torch.load(f'Zebra/data/E{i}.pt')\n",
    "        for data in test_dataset:\n",
    "            i+=1\n",
    "            # keep track of progress\n",
    "            if i % 100 == 0:\n",
    "                print(f'TESTING ON MOLECULE {i}')\n",
    "                print(f'MOST RECENT LOSSES: {fcle_losses[-1]}, {fcse_losses[-1]}, {pcle_losses[-1]}')\n",
    "            U = data.y\n",
    "            # calculate loss of each model and save in list\n",
    "            loss_item = loss_fn(models[0](data),U).item()\n",
    "            wandb.log(loss_item)\n",
    "            fcse_losses.append(loss_fn(models[1](data),U).item())\n",
    "            pcle_losses.append(loss_fn(models[2](data),U).item())\n",
    "\n",
    "# save losses to machine because it took a couple of hours to compute\n",
    "torch.save(Tensor(fcle_losses),'Zebra/losses/fcle_losses.pt')\n",
    "torch.save(Tensor(fcse_losses),'Zebra/losses/fcse_losses.pt')\n",
    "torch.save(Tensor(pcle_losses),'Zebra/losses/pcle_losses.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
      "Extracting ./raw/qm9_v3.zip\n",
      "Processing...\n",
      "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = QM9('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(data, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader)).batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
